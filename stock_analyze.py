import streamlit as st
import pandas as pd
import numpy as np
import yfinance as yf
import gspread
import google.generativeai as genai
import requests
from bs4 import BeautifulSoup
from google.oauth2.service_account import Credentials
from datetime import datetime
import time
import os
import random
import urllib3

# --- åŸºç¤é…ç½® ---
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
st.set_page_config(page_title="AI è‚¡å¸‚å°ˆå®¶ v16.5", layout="wide")

# ç’°å¢ƒåƒæ•¸è¨­å®š
SHEET_NAME = "Stock_Predictions_History"
CREDENTIALS_JSON = "eco-precept-485904-j5-7ef3cdda1b03.json"

# AI é…ç½® (è«‹åœ¨ Secrets è¨­å®š GEMINI_API_KEY)
GEMINI_API_KEY = st.secrets.get("GEMINI_API_KEY", "YOUR_KEY_HERE")
genai.configure(api_key=GEMINI_API_KEY)
ai_model = genai.GenerativeModel('gemini-1.5-flash')

# ==================== 1. é›²ç«¯é€£ç·šæ¨¡çµ„ (ä¿®å¾© Illegal header å ±éŒ¯) ====================

def get_gspread_client():
    """ä¿®å¾©éæ³•å­—å…ƒå°è‡´çš„æˆæ¬Šå ±éŒ¯"""
    scopes = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']
    try:
        if "gcp_service_account" in st.secrets:
            creds_info = dict(st.secrets["gcp_service_account"])
            # é—œéµä¿®æ­£ï¼šå¼·åˆ¶è½‰ç¾©æ›è¡Œç¬¦è™Ÿï¼Œé˜²æ­¢ Header é©—è­‰å¤±æ•—
            creds_info["private_key"] = creds_info["private_key"].replace("\\n", "\n")
            creds = Credentials.from_service_account_info(creds_info, scopes=scopes)
        elif os.path.exists(CREDENTIALS_JSON):
            creds = Credentials.from_service_account_file(CREDENTIALS_JSON, scopes=scopes)
        else:
            return None
        return gspread.authorize(creds)
    except Exception as e:
        st.error(f"âŒ æˆæ¬Šå¤±æ•—: {e}")
        return None

def get_top_100_tickers():
    """æ­¥é©Ÿ 1ï¼šæŠ“å– EXCEL ç¬¬ä¸€é çš„å‰ 100 æ”¯è‚¡ç¥¨"""
    client = get_gspread_client()
    if not client: return []
    try:
        sh = client.open(SHEET_NAME)
        ws = sh.get_worksheet(0)
        df = pd.DataFrame(ws.get_all_records())
        return df['è‚¡ç¥¨ä»£è™Ÿ'].dropna().astype(str).head(100).tolist()
    except Exception as e:
        st.error(f"è®€å–æ¸…å–®å¤±æ•—: {e}")
        return []

# ==================== 2. å¤šç¶­åº¦åˆ†æèˆ‡çˆ¬èŸ²æ¨¡çµ„ ====================

def crawl_news_for_ai(symbol):
    """æ­¥é©Ÿ 2-äºŒï¼šçˆ¬èŸ²å››å¤§æ–°èç¶²æœå°‹æ¨™çš„ç›¸é—œæ–°è"""
    stock_id = symbol.split('.')[0]
    headers = {'User-Agent': 'Mozilla/5.0'}
    # æœå°‹ç›®æ¨™ï¼šFTNNã€èšè²¡ç¶²ã€é‰…äº¨ç¶²ã€ç¶“æ¿Ÿæ—¥å ± (ç¯„ä¾‹æ•´åˆç¶²å€)
    news_sources = [f"https://news.cnyes.com/news/cat/tw_stock_news"]
    combined_news = ""
    try:
        res = requests.get(news_sources[0], headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        titles = [t.get_text() for t in soup.find_all(['h3', 'a']) if stock_id in t.get_text()]
        combined_news = " ".join(titles[:5])
    except: pass
    return combined_news if combined_news else "ç„¡é‡å¤§å³æ™‚æ–°è"

def get_factor_score(ticker, ticker_df):
    """æ­¥é©Ÿ 2-ä¸‰ï¼šåŸºæœ¬é¢èˆ‡æŠ€è¡“é¢ç©åˆ†åˆ†æ"""
    score = 0
    try:
        # 1. æŠ€è¡“é¢ï¼šå‡ç·šé»ƒé‡‘äº¤å‰åˆ¤å®š
        ma5 = ticker_df['Close'].rolling(5).mean().iloc[-1]
        ma20 = ticker_df['Close'].rolling(20).mean().iloc[-1]
        if ma5 > ma20: score += 2
        
        # 2. åŸºæœ¬é¢ï¼šæœ¬ç›Šæ¯”è³‡è¨Š (yfinance)
        info = yf.Ticker(ticker).info
        if info.get('forwardPE', 100) < 15: score += 1
    except: pass
    return score

# ==================== 3. ä¸»åŸ·è¡Œæµç¨‹ (æŠ—å°é–ç‰ˆ) ====================

st.title("ğŸ›¡ï¸ AI è‚¡å¸‚å…¨èƒ½å°ˆå®¶ v16.5")

if st.button("ğŸš€ å•Ÿå‹• Top 100 å…¨æ–¹ä½ AI é æ¸¬ä»»å‹™"):
    tickers = get_top_100_tickers()
    client = get_gspread_client()
    
    if client and tickers:
        sh = client.open(SHEET_NAME)
        ws = sh.get_worksheet(0)
        p_bar = st.progress(0)
        status = st.empty()
        
        # æ‰¹é‡ç²å–æ•¸æ“šä»¥æ¸›å°‘è«‹æ±‚æ¬¡æ•¸
        status.text("æ­£åœ¨æ‰¹é‡åŒæ­¥å¸‚å ´æ­·å²æ•¸æ“š...")
        all_hist = yf.download(tickers, period="3mo", group_by='ticker', threads=True, progress=False)
        
        for idx, t in enumerate(tickers):
            try:
                status.text(f"åˆ†æä¸­ ({idx+1}/100): {t}")
                
                # æå–å€‹è‚¡æ•¸æ“š
                df = all_hist[t].dropna() if isinstance(all_hist.columns, pd.MultiIndex) else all_hist.dropna()
                if df.empty: continue
                
                curr_p = round(float(df['Close'].iloc[-1]), 2)
                factor_score = get_factor_score(t, df)
                news_txt = crawl_news_for_ai(t)
                
                # æ­¥é©Ÿ 2-äºŒï¼šä¸Ÿçµ¦ Gemini åˆ†æç©åˆ†ä¸¦é æ¸¬èµ°å‹¢
                prompt = f"åˆ†æ{t}ã€‚ç¾åƒ¹{curr_p}ã€‚åˆ†æåˆ†{factor_score}ã€‚æ–°èï¼š{news_txt}ã€‚è«‹é æ¸¬æœªä¾†5æ—¥æ”¶ç›¤åƒ¹ã€‚è«‹åƒ…å›å‚³ï¼šåƒ¹1,åƒ¹2,åƒ¹3,åƒ¹4,åƒ¹5"
                response = ai_model.generate_content(prompt)
                preds = [float(p) for p in response.text.strip().split(',')]
                
                # å¯«å…¥ Excel E-J æ¬„
                # E-I: é æ¸¬åƒ¹, J: èª¤å·®% (è¨­ç‚ºå¾…å®š)
                ws.update(f"E{idx+2}:J{idx+2}", [preds + ["-"]])
                
                # æ™ºèƒ½å†·å»é é˜²å°é–
                time.sleep(random.uniform(1.0, 2.0))
                if (idx + 1) % 10 == 0:
                    status.text("å†·å»ä¸­ï¼Œé¿å…è§¸ç™¼ Too Many Requests...")
                    time.sleep(15)
                    
            except Exception as e:
                st.warning(f"è·³é {t}: {e}")
                
            p_bar.progress((idx + 1) / len(tickers))
            
        status.text("âœ… å…¨éƒ¨ä»»å‹™åŸ·è¡Œå®Œç•¢")
        st.success("ğŸ‰ é æ¸¬çµæœå·²æˆåŠŸåŒæ­¥è‡³ Excel E-J æ¬„ä½ï¼")
