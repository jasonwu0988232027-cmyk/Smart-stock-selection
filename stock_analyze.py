import streamlit as st
import yfinance as yf
import pandas as pd
import pandas_ta as ta
import time
import random
import requests
import urllib3
import json
import os
import gspread
from datetime import datetime
from google.oauth2.service_account import Credentials

# --- åŸºç¤é…ç½® ---
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
st.set_page_config(page_title="å°è‚¡å¤šå› å­æ±ºç­–ç³»çµ± (é›²ç«¯åŒæ­¥ç‰ˆ)", layout="wide")

# Google Sheets é…ç½®
SHEET_NAME = "Stock_Predictions_History" 
# è®€å–ä¸Šå‚³çš„é‡‘é‘°æª”æ¡ˆåç¨±
CREDENTIALS_JSON = "eco-precept-485904-j5-7ef3cdda1b03.json" 

# --- Google Sheets æˆæ¬Šé‚è¼¯ ---
def get_gspread_client():
    """
    å»ºç«‹ Google Sheets API æˆæ¬Šå®¢æˆ¶ç«¯
    """
    scopes = [
        'https://www.googleapis.com/auth/spreadsheets',
        'https://www.googleapis.com/auth/drive'
    ]
    
    # å„ªå…ˆå¾ Streamlit Secrets è®€å–ï¼Œå¦å‰‡è®€å–æœ¬åœ° JSON
    if "gcp_service_account" in st.secrets:
        try:
            creds_dict = dict(st.secrets["gcp_service_account"])
            creds = Credentials.from_service_account_info(creds_dict, scopes=scopes)
            return gspread.authorize(creds)
        except Exception as e:
            st.error(f"Secrets Authorization Failed: {e}")
            return None
    elif os.path.exists(CREDENTIALS_JSON):
        try:
            creds = Credentials.from_service_account_file(CREDENTIALS_JSON, scopes=scopes)
            return gspread.authorize(creds)
        except Exception as e:
            st.error(f"Local JSON Authorization Failed: {e}")
            return None
    return None

def save_to_sheets(new_data, sheet_index=0):
    """
    å°‡è³‡æ–™å¯«å…¥ Google Sheets
    """
    client = get_gspread_client()
    if client is None:
        st.error("âš ï¸ Cannot connect to Google Sheets. Check credentials.")
        return False
        
    try:
        sh = client.open(SHEET_NAME)
        all_ws = sh.worksheets()
        if len(all_ws) > sheet_index:
            target_ws = all_ws[sheet_index]
        else:
            target_ws = sh.add_worksheet(title=f"Market_Scan_{datetime.now().strftime('%Y%m%d')}", rows=1000, cols=10)
        
        # æª¢æŸ¥ä¸¦å¯«å…¥è¡¨é ­ (é‡å°æœ¬æ¬¡å…¨å¸‚å ´æƒææ ¼å¼)
        if not target_ws.acell('A1').value:
            headers = ["æƒææ—¥æœŸ", "è‚¡ç¥¨ä»£è™Ÿ", "æ”¶ç›¤åƒ¹", "æˆäº¤å€¼(å„„)"]
            target_ws.append_row(headers)
             
        target_ws.append_rows(new_data)
        return True
    except Exception as e:
        st.error(f"âŒ Cloud Sync Failed: {str(e)}")
        return False

# --- è‚¡ç¥¨åˆ†æé‚è¼¯ ---
@st.cache_data(ttl=86400)
def get_full_market_tickers():
    """
    ç²å–å°è‚¡ä¸Šå¸‚è‚¡ç¥¨ä»£è™Ÿ
    """
    url = "https://isin.twse.com.tw/isin/C_public.jsp?strMode=2"
    try:
        res = requests.get(url, timeout=10, verify=False, headers={'User-Agent': 'Mozilla/5.0'})
        res.encoding = 'big5'
        df = pd.read_html(res.text)[0]
        df.columns = df.iloc[0]
        df = df[df['æœ‰åƒ¹è­‰åˆ¸ä»£è™ŸåŠåç¨±'].str.contains("  ", na=False)]
        tickers = [f"{t.split('  ')[0].strip()}.TW" for t in df['æœ‰åƒ¹è­‰åˆ¸ä»£è™ŸåŠåç¨±'] if len(t.split('  ')[0].strip()) == 4]
        return tickers
    except:
        return [f"{i:04d}.TW" for i in range(1101, 1200)] # å¤±æ•—æ™‚çš„å›é€€æ©Ÿåˆ¶

# --- UI ä»‹é¢ ---
st.title("ğŸ† å…¨å¸‚å ´è³‡é‡‘æŒ‡æ¨™æ’è¡Œèˆ‡é›²ç«¯åŒæ­¥")

if st.button("ğŸš€ åŸ·è¡Œæ·±åº¦æƒæä¸¦åŒæ­¥è‡³é›²ç«¯"):
    all_list = get_full_market_tickers()
    res_rank = []
    upload_data = [] # æº–å‚™ä¸Šå‚³è‡³ Sheets çš„æ ¼å¼
    
    p_bar = st.progress(0, text="æ­£åœ¨åˆ†æå…¨å¸‚å ´æˆäº¤å€¼...")
    
    # ç‚ºäº†æ¼”ç¤ºèˆ‡é€Ÿåº¦ï¼Œç¯„ä¾‹åƒ…æŠ“å–å‰ 50 æª”ï¼Œæ­£å¼ä½¿ç”¨å¯ç§»é™¤åˆ‡ç‰‡ [:50]
    scan_list = all_list[:50] 
    batch_size = 10
    
    for i in range(0, len(scan_list), batch_size):
        batch = scan_list[i : i + batch_size]
        try:
            # æ‰¹é‡ä¸‹è¼‰æ•¸æ“š
            data = yf.download(batch, period="2d", group_by='ticker', threads=True, progress=False)
            current_date = datetime.now().strftime('%Y-%m-%d')
            
            for t in batch:
                try:
                    t_df = data[t].dropna() if isinstance(data.columns, pd.MultiIndex) else data.dropna()
                    if not t_df.empty:
                        last = t_df.iloc[-1]
                        price = float(last['Close'])
                        val = (price * float(last['Volume'])) / 1e8
                        
                        res_rank.append({"è‚¡ç¥¨ä»£è™Ÿ": t, "æ”¶ç›¤åƒ¹": price, "æˆäº¤å€¼(å„„)": val})
                        # æ§‹å»º Google Sheets åˆ—è³‡æ–™
                        upload_data.append([current_date, t, price, round(val, 2)])
                except: continue
        except: pass
        p_bar.progress(min((i + batch_size) / len(scan_list), 1.0))
        time.sleep(random.uniform(0.1, 0.5))

    if res_rank:
        df_result = pd.DataFrame(res_rank).sort_values("æˆäº¤å€¼(å„„)", ascending=False)
        st.subheader("æœ¬æ—¥æƒæçµæœ (Top 50)")
        st.dataframe(df_result, use_container_width=True)
        
        # åŸ·è¡Œé›²ç«¯åŒæ­¥
        st.info("æ­£åœ¨åŒæ­¥è³‡æ–™è‡³ Google Sheets...")
        if save_to_sheets(upload_data):
            st.success(f"âœ… å·²æˆåŠŸå°‡ {len(upload_data)} ç­†è³‡æ–™åŒæ­¥è‡³è©¦ç®—è¡¨: {SHEET_NAME}")
